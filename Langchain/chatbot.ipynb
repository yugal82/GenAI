{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000192D7EAA2C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000192D7EA8C40>, model_name='Gemma2-9b-It', groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm_model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n",
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Yugal.  \\n\\nI remember that you told me at the beginning of our conversation. ðŸ˜Š \\n\\n\\n\\n', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 98, 'total_tokens': 125, 'completion_time': 0.054, 'prompt_time': 0.016826839, 'queue_time': None, 'total_time': 0.070826839}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-306ef614-23d6-4243-b8b7-401760ea3411-0', usage_metadata={'input_tokens': 98, 'output_tokens': 27, 'total_tokens': 125})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "# llm_model.invoke([HumanMessage(content=\"Hi, my name is Yugal and I am incoming grad student at USC\")])\n",
    "llm_model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, my name is Yugal and I am incoming grad student at USC\"),\n",
    "        AIMessage(content=\"Hi Yugal,\\n\\nWelcome to USC!  That's exciting!\\n\\nWhat program will you be joining?  \\n\\nI'm happy to help with any questions you might have about your time at USC, or just chat about your interests. \\n\\n\"),\n",
    "        HumanMessage(content=\"Hey, what is my name?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message history\n",
    "We can use Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Further interactions will then load those messages and pass them into the chain as part of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "        \n",
    "    return store[session_id]\n",
    "\n",
    "with_msg_history = RunnableWithMessageHistory(llm_model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Yugal, it's great to meet you!  Being a Software Engineer at a company like Amazon, Microsoft, Google, or Apple is really something.  \\n\\nCould you tell me a little bit about what you work on?  I'm curious to hear about the kind of projects you're involved in.  \\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_msg_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is yugal, and I am a Software Engineer at Amazon/Microsoft/Google/Apple\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You said your name is Yugal and that you are a Software Engineer at one of these companies:\\n\\n* Amazon\\n* Microsoft\\n* Google\\n* Apple \\n\\n\\nLet me know if you'd like to tell me which one! ðŸ˜Š \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 227, 'total_tokens': 281, 'completion_time': 0.108359015, 'prompt_time': 0.020072257, 'queue_time': None, 'total_time': 0.128431272}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-4d6eca81-a4cd-4829-935c-a3691674c60e-0', usage_metadata={'input_tokens': 227, 'output_tokens': 54, 'total_tokens': 281})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_msg_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name and where do I work?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and no access to personal information about you, including your name or your workplace.\\n\\nIf you'd like to tell me your name and where you work, I'd be happy to know! ðŸ˜Š\\n\", response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 24, 'total_tokens': 79, 'completion_time': 0.110383174, 'prompt_time': 0.002544888, 'queue_time': None, 'total_time': 0.112928062}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-490d9c2f-cf69-4728-afc0-6df081999720-0', usage_metadata={'input_tokens': 24, 'output_tokens': 55, 'total_tokens': 79})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the config and see whether it tries to remember the chat history of chat1. (It should not as the session_id is changed and a new chat history will be created)\n",
    "config = {\"configurable\": {\"session_id\": \"chat2\"}}\n",
    "with_msg_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name and where do I work?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi my name is yugal, and I am a Software Engineer at Amazon/Microsoft/Google/Apple'), AIMessage(content=\"Hi Yugal, it's nice to meet you!\\n\\nThat's impressive, working as a Software Engineer at one of those tech giants! Which one is it, if you don't mind me asking?  \\n\\nI'm always interested in hearing about people's experiences in the tech industry. What kind of work do you do? \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 35, 'total_tokens': 111, 'completion_time': 0.153623564, 'prompt_time': 0.004633226, 'queue_time': None, 'total_time': 0.15825678999999998}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-7eb37ded-9da5-43d7-940b-15842c3c320d-0', usage_metadata={'input_tokens': 35, 'output_tokens': 76, 'total_tokens': 111}), HumanMessage(content='Hi my name is yugal, and I am a Software Engineer at Amazon/Microsoft/Google/Apple'), AIMessage(content=\"Hi Yugal, it's great to meet you!  Being a Software Engineer at a company like Amazon, Microsoft, Google, or Apple is really something.  \\n\\nCould you tell me a little bit about what you work on?  I'm curious to hear about the kind of projects you're involved in.  \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 139, 'total_tokens': 210, 'completion_time': 0.14287195, 'prompt_time': 0.012445642, 'queue_time': None, 'total_time': 0.155317592}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-d26fa80c-df14-4aeb-a14f-f1a97355c2d4-0', usage_metadata={'input_tokens': 139, 'output_tokens': 71, 'total_tokens': 210}), HumanMessage(content='What is my name and where do I work?'), AIMessage(content=\"You said your name is Yugal and that you are a Software Engineer at one of these companies:\\n\\n* Amazon\\n* Microsoft\\n* Google\\n* Apple \\n\\n\\nLet me know if you'd like to tell me which one! ðŸ˜Š \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 227, 'total_tokens': 281, 'completion_time': 0.108359015, 'prompt_time': 0.020072257, 'queue_time': None, 'total_time': 0.128431272}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-4d6eca81-a4cd-4829-935c-a3691674c60e-0', usage_metadata={'input_tokens': 227, 'output_tokens': 54, 'total_tokens': 281})]),\n",
       " 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is my name and where do I work?'), AIMessage(content=\"As an AI, I have no memory of past conversations and no access to personal information about you, including your name or your workplace.\\n\\nIf you'd like to tell me your name and where you work, I'd be happy to know! ðŸ˜Š\\n\", response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 24, 'total_tokens': 79, 'completion_time': 0.110383174, 'prompt_time': 0.002544888, 'queue_time': None, 'total_time': 0.112928062}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-490d9c2f-cf69-4728-afc0-6df081999720-0', usage_metadata={'input_tokens': 24, 'output_tokens': 55, 'total_tokens': 79})])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message which we are passing to the LLM. \n",
    "First, lets add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all the question to the best of your ability.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|llm_model\n",
    "chain.invoke({\"messages\": [HumanMessage(content=\"Hi, my name is Yugal\")]})\n",
    "with_msg_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Yugal, it's nice to meet you! \\n\\nWhat can I do for you today? ðŸ˜Š  \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 81, 'total_tokens': 109, 'completion_time': 0.056, 'prompt_time': 0.012488728, 'queue_time': None, 'total_time': 0.068488728}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-206f2f75-739d-434d-9fee-6dea79ca7452-0', usage_metadata={'input_tokens': 81, 'output_tokens': 28, 'total_tokens': 109})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response = with_msg_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is Yugal\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant. Answer all the question to the best of your ability in {language}.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "complex_chain = complex_prompt|llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Â¡Hola Yugal! Mucho gusto en conocerte. Â¿CÃ³mo puedo ayudarte hoy? ðŸ˜Š \\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = complex_chain.invoke({\"messages\": [HumanMessage(content=\"Hi my name is Yugal\")], \"language\": \"Spanish\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, becuase there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_msg_history = RunnableWithMessageHistory(complex_chain, get_session_history, input_messages_key=\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Â¡Hola Yugal! Â¿CÃ³mo estÃ¡s? ðŸ˜Š  \\n\\nÂ¿En quÃ© te puedo ayudar hoy? \\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "response = with_msg_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hi i am Yugal\")],\n",
    "        \"language\": \"Spanish\"\n",
    "    },\n",
    "    config = config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tu nombre es Yugal. ðŸ˜Š  \\n\\nÂ¿Hay algo mÃ¡s que quieras saber? \\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_msg_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What is my name?\")],\n",
    "        \"language\": \"Spanish\"\n",
    "    },\n",
    "    config = config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation history\n",
    "\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window.of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "\n",
    "\n",
    "`trim_messages` helps to reduce the how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with the other parameters like if we want to always keep the system message and whether to allow partial messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an expert assistant'),\n",
       " HumanMessage(content='what is 2+2'),\n",
       " AIMessage(content='2+2 = 4'),\n",
       " HumanMessage(content='Thanks'),\n",
       " AIMessage(content='No problem, what more can I help you with!'),\n",
       " HumanMessage(content='having fun'),\n",
       " AIMessage(content='Yes!')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer = trim_messages(max_tokens=45, strategy=\"last\", token_counter=llm_model, include_system=True, allow_partial=False, start_on=\"human\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert assistant\"),\n",
    "    HumanMessage(content=\"Hi, I am John Doe\"),\n",
    "    AIMessage(content=\"Hey John Doe, how can I help you!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"Nice\"),\n",
    "    HumanMessage(content=\"what is 2+2\"),\n",
    "    AIMessage(content=\"2+2 = 4\"),\n",
    "    HumanMessage(content=\"Thanks\"),\n",
    "    AIMessage(content=\"No problem, what more can I help you with!\"),\n",
    "    HumanMessage(content=\"having fun\"),\n",
    "    AIMessage(content=\"Yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 + 2 = 4 ðŸ˜Š  \\n\\nIs there anything else I can help you with?  \\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)|prompt|llm_model)\n",
    "response = chain.invoke({\"messages\": messages + [HumanMessage(content=\"What is 2+2\")]})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 + 2 = 4  ðŸ˜Š \\n\\nIs there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_msg_history = RunnableWithMessageHistory(chain, get_session_history, input_messages_key=\"messages\")\n",
    "config = {\"configurable\": {\"session_id\": \"chat5\"}}\n",
    "\n",
    "response = with_msg_history.invoke(\n",
    "    {\"messages\": messages + [HumanMessage(content=\"What is 2+2\")]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
